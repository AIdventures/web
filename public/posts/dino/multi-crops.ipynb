{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale range of the cropped image before resizing,\n",
    "# relatively to the origin image. Used for large global view cropping\n",
    "global_crops_scale=(0.4, 1.)\n",
    "# Number of small local views to generate\n",
    "local_crops_number=8  \n",
    "# Scale range of the cropped image before resizing,\n",
    "# relatively to the origin image. Used for small local view cropping of multi-crop.\n",
    "local_crops_scale=(0.05, 0.4)\n",
    "\n",
    "class DataAugmentationDINO(object):\n",
    "    \"\"\" Data augmentation strategy used in DINO\n",
    "    Some augmentation were removed for simplicity\n",
    "    \"\"\"\n",
    "    def __init__(self, global_crops_scale, local_crops_scale, local_crops_number):\n",
    "        flip_and_color_jitter = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomApply(\n",
    "                [\n",
    "                    transforms.ColorJitter(\n",
    "                        brightness=0.4,\n",
    "                        contrast=0.4,\n",
    "                        saturation=0.2,\n",
    "                        hue=0.1\n",
    "                    )\n",
    "                ],\n",
    "                p=0.8\n",
    "            ),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "        ])\n",
    "        normalize = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        ])\n",
    "\n",
    "        # first global crop\n",
    "        self.global_transfo1 = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(\n",
    "                224, scale=global_crops_scale, interpolation=Image.BICUBIC\n",
    "            ),\n",
    "            flip_and_color_jitter,\n",
    "            #utils.GaussianBlur(1.0),\n",
    "            normalize,\n",
    "        ])\n",
    "        # second global crop\n",
    "        self.global_transfo2 = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(\n",
    "                224, scale=global_crops_scale, interpolation=Image.BICUBIC\n",
    "            ),\n",
    "            flip_and_color_jitter,\n",
    "            #utils.GaussianBlur(0.1),\n",
    "            #utils.Solarization(0.2),\n",
    "            normalize,\n",
    "        ])\n",
    "        # transformation for the local small crops\n",
    "        self.local_crops_number = local_crops_number\n",
    "        self.local_transfo = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(\n",
    "                96, scale=local_crops_scale, interpolation=Image.BICUBIC\n",
    "            ),\n",
    "            flip_and_color_jitter,\n",
    "            #utils.GaussianBlur(p=0.5),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "    def __call__(self, image):\n",
    "        crops = []\n",
    "        crops.append(self.global_transfo1(image))\n",
    "        crops.append(self.global_transfo2(image))\n",
    "        for _ in range(self.local_crops_number):\n",
    "            crops.append(self.local_transfo(image))\n",
    "        return crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"cat.jpg\"\n",
    "with open(image_path, 'rb') as f:\n",
    "    img = Image.open(f)\n",
    "    img = img.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply data augmentation\n",
    "data_augmentation = DataAugmentationDINO(global_crops_scale, local_crops_scale, local_crops_number)\n",
    "crops = data_augmentation(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"crops\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "for idx, crop in enumerate(crops):\n",
    "    # convert to PIL image\n",
    "    crop = transforms.ToPILImage()(crop)\n",
    "    crop.save(os.path.join(SAVE_DIR, f\"crop_{idx}.jpg\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
