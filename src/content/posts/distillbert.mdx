---
author: Mario Parre√±o
date: 02/10/2019
image: ./images/distillbert/lakeside.jpg
tags: ["nlp", "transformer", "paper", "encoder"]
title: DistillBERT - A distilled version of BERT
description: A distilled version of BERT that is smaller, faster, cheaper and lighter retaining most of its performance.
draft: false
---

import Underlined from "../../components/blog/Underlined.astro";
import SEOImage from "../../components/blog/SEOImage.astro";

Research on pre-trained models indicatest that training even larger models still
leads to better performances on downstream tasks. However, operating large-scale
pre-trained models in on-the-edge and/or under constrained computational
training or inference budgets remains challenging.

DistillBERT proposes a method to pre-train a smaller general-purpose language representation model.
Through knowledge distillation during the pre-training phase authors show that is possible to 
<Underlined>reduce the size of a [BERT](/blog/bert) model by 40% while rertaining 97% of its language understanding capabilities and being 60% faster.</Underlined>
To achieve this, authors introduce a
<Underlined color="red">triple loss combining language modeling, distillation and cosine-distance losses</Underlined>.

## Architecture

DistilBERT uses a student-teacher framework, where the student is trained to mimic the teacher's
predictions, also known as knowledge distillation.

The teacher is a $BERT_{BASE}$ model. The student, DistillBERT, has the same general architecture
but the <Underlined>*token-type embeddings* and the *pooler* are removed</Underlined>
while the number of layers is reduced  by a factor of 2. Investigations show that variations in the hiddens
size dimension have a smaller impact on computation effiency,
so <Underlined>DistilBERT focus on reducing the number of layers</Underlined>.

An important element in the training procedure is to find the right initialization for the student to converge.
Taking advantage of the common dimensionality between teacher and student networks,
<Underlined color="green">DistilBERT initializes the student's weights from the teacher by taking one layer out of two</Underlined>.

Finally, following previous approaches like [RoBERTa](/blog/roberta), DistilBERT uses a dynamic masking strategy
without the next sentence prediction objective. Also, the student is distilled on very large batches
leveraging gradient accumulation, up to 4K examples per batch.

<SEOImage
  src={"/posts/distillbert/overview.png"}
  caption="DistillBERT model overview. The student is trained to mimic the teacher's predictions. A triple loss combining language modeling, distillation and cosine-distance losses is used to train the student."
  alt="DistillBERT model overview. The student is trained to mimic the teacher's predictions. A triple loss combining language modeling, distillation and cosine-distance losses is used to train the student."
  width={900}
  height={450}
  format="webp"
  className="w-full"
/>

### Training Losses

To leverage the inductive biases learned by larger models during pre-training, DistillBERT
introduces a triple loss combining masked language modeling, distillation and cosine-distance losses.
The final loss is a linear combination of the three losses.

#### Masked Language Modeling

As in the original [BERT](/blog/bert) model, DistillBERT uses a masked language model (MLM) to
train the model to predict the original vocabulary id of the masked word based only on its context.


#### Knowledge Distillation

Knowledge distillation is a compression technique in which a compact model- <Underlined color="green">the student</Underlined> - is trained
to <Underlined>reproduce the behaviour</Underlined> of a larger model - <Underlined color="green">the teacher</Underlined> - or an ensemble of models.

**The teacher** is first trained to predict an instnace class by maximizing the estimated probability of gold labels.
A standard objective thus involves minimizing the cross-entropy between the model's predicted distribution and 
the one-hot empirical distribution of training labels. A model performing well on the training set will predict
an output distribution with high probability on the correct class and with <Underlined color="green">near-zero</Underlined>
probabilities on other classes. But <Underlined>some of these *near-zero* probabilities are larger than others</Underlined> and reflect,
in part, the generalization of the model and how well it will perform on the test set.

**The student** is then trained with a distillation loss ofver the
<Underlined color="red">soft target probabilities of the teacher</Underlined>. 

#### Cosine-Distance

Authors found beneficial to add a cosine embedding loss which tends to align the directions of the student and teacher hidden states.
This way, the student is likely not only to reproduce masked tokens correctly but also to construct
embeddings that are similar to those of the teacher.
Pytorch already includes a <a href="https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html#torch.nn.CosineEmbeddingLoss" target="_blank">cosine embedding loss</a>.

## Credits

- [Paper - DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)
- [Pytorch Tutorial about Knowledge Distillation & Cosine Embedding Loss](https://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html)
- [Vyacheslav Efimov Medium Post about DistillBERT](https://towardsdatascience.com/distilbert-11c8810d29fc)