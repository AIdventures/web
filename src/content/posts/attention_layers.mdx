---
author: Mario Parre√±o
date: 11/09/2024
image: ./images/speeding_attention/snowy.jpg
tags: ["transformers", "nlp", "attention", "optimization", "cache"]
title: Speeding up Attention Layers
description: Multi-head, Multi-Query & Grouped-Query Attention layers clearly explained. How cache works in the Attention layers.
---
import Underlined from "../../components/blog/Underlined.astro"
import SEOImage from "../../components/blog/SEOImage.astro"


This post delves into the fascinating world of attention layers,
exploring their evolution from the base multi-head attention to more recent variants
like multi-query and grouped-query attention.
We'll examine why these newer architectures appeared
and how they address the limitations of their predecessors.

As advance, the <Underlined>inference complexities</Underlined>, spatial and temporal,
are the core of the improvements, focusing on how to <Underlined>reduce the cache</Underlined> needed
without losing the performance of the model.


## Multi-Head Attention

### KV-Cache

## Multi-Query Attention

## Grouped-Query Attention



## Credits

- [Multi-Query & Grouped-Query Attention - Tinkerd](https://tinkerd.net/blog/machine-learning/multi-query-attention/)
